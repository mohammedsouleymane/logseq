- lecture 1
  collapsed:: true
	- ### **Overview of Next-Generation User Interfaces**
	  
	  The lecture presents a vision of user interfaces that move well beyond traditional command-based interactions and classic graphical user interfaces (GUIs). It explains that advances in technology—ranging from gesture and speech recognition to wearable computing and mixed reality—have dramatically broadened the range of possible interface types. This evolution has sparked a rethinking of what it means for an interface to be “natural.”
	  
	  ---
	- ### **Diversity of Interface Types**
	  
	  A critical insight is that thousands of interaction modalities now exist. The lecture outlines the spectrum of interfaces that include, but are not limited to:
	- **Traditional Models:** Command-based systems and WIMP/GUI interfaces.
	- **Multimedia and Mobile:** Systems that integrate audio, video, and touch to suit on-the-go usage.
	- **Emerging Paradigms:** Virtual reality (VR), augmented and mixed reality, tangible interfaces, and even brain–computer and wearable interfaces.
	  
	  Rather than prescribing a one-size-fits-all solution, the discussion emphasizes that the choice of interface must be driven by the specific task, user needs, and context. For example, while a speech-based system might feel natural in some settings, it may not be optimal for all situations compared to a physical gesture or tactile input.
	  
	  ---
	- ### **Questioning Naturalness in Interaction**
	  
	  An essential theme of the lecture is the challenge of designing truly "natural" interfaces. It questions common assumptions—for instance:
	- Is it inherently more intuitive to give a verbal command (e.g., “open”) rather than to perform a physical action like flicking a switch?
	- Do full-body gestures compare favorably to traditional remote control inputs?
	  
	  These reflections indicate that what is considered "natural" often depends on context and the specific domain of usage, and that designers must weigh the benefits and drawbacks of each interaction method.
	  
	  ---
	- ### **Principles of Interaction Design**
	  
	  The lecture advocates a user-centered, iterative approach to interface design that is grounded in clarity, creativity, and comprehensive evaluation. Key design principles include: (EDPE)
	- **User-Centered Design:** Starting with a deep understanding of who the users are, what tasks they need to perform, and in what context they operate.
	- **1. Establishing Requirements:** Identifying not only the functional needs of an interface but also considering environmental factors, data requirements, and the user’s characteristics.
	- **2. Designing Alternatives:** Encouraging the exploration of multiple design alternatives rather than settling for “good enough.” Creative brainstorming and iterative refinement are stressed as critical.
	- **3. Prototyping:** From low-fidelity sketches and paper prototypes to more advanced simulations or wizard-of-oz experiments, early prototyping is vital for uncovering design flaws and gaining user feedback.
	- **4. Evaluation:** A strong emphasis is placed on continuous and thorough evaluation processes. This involves both usability testing in controlled environments and observing real-world interactions to ensure that the interface meets its usability and user experience goals.
	  
	  ---
	- ### **Future Outlook**
	  
	  In conclusion, the lecture points toward a future where user interfaces are not merely tools for completing digital tasks but become extensions of human cognition and experience. By seamlessly blending physical and digital realms and leveraging multimodal input (from voice to touch to gesture), next-generation interfaces promise richer, more intuitive, and context-aware interactions. Designers are encouraged to think broadly and creatively, continually iterating on their designs to align ever more closely with the dynamic nature of human behavior.
- lecture 2
  collapsed:: true
	- ### 1. Foundations of Information Architectures
	  
	  The lecture begins by defining the scope of Information Architectures (IAs) as the study and practice of organizing, structuring, and labeling content. Professor Signer emphasizes that a robust IA is not just about storage; it involves modeling entire information spaces with attention to linking, navigation, search, and presentation. This approach focuses on the interplay between *content*, *users*, and *context*—a triad that underpins effective digital as well as physical information systems.
	  
	  ---
	- ### 2. Personal Information Management (PIM)
	  
	  A substantial portion of the lecture is devoted to Personal Information Management (PIM). PIM is described as both the practice and the scholarly study of how individuals keep, organize, retrieve, and use information—for work, personal life, and multiple roles. Two classic organizational strategies are highlighted:
	- **Filers**: These use systematic, titled, and hierarchical arrangements (akin to well-organized file systems).
	- **Pilers**: In contrast, piles emerge more organically, relying on cues like spatial location or access frequency rather than explicit structure.
	  
	  This distinction, originally drawn from office studies (e.g., Malone’s research), is extended to the digital world—informing how emails, bookmarks, and files are managed.
	  
	  ---
	- ### 3. Memory Models and the Limits of Paper Simulation
	  
	  The lecture then transitions into memory models by drawing an analogy with human long-term memory. It explains that—just as our brain stores episodic, semantic, and procedural information—digital systems must address challenges of permanence and retrieval.
	  
	  Professor Signer revisits Vannevar Bush’s seminal essay, *As We May Think* (1945), which introduced the idea of the Memex, a system based on associative indexing rather than rigid, linear filing. This historical insight critiques the dominant paper-simulation paradigm in digital document design—specifically the “What You See Is What You Get” (WYSIWYG) approach. According to the lecture, this simulation “freezes” documents into closed, rectangular objects that lack the flexibility of real paper (no marginal notes, overlays, or dynamic annotations).
	  
	  ---
	- ### 4. Beyond the Desktop Metaphor: Lifestreams and Cross-Media Approaches
	  
	  Moving on, the lecture explores alternative models that break away from the paper simulation:
	- **Lifestreams**: Instead of compartmentalizing information into static files or folders, a time-ordered stream can represent the past, present, and future. This model leverages the natural human tendency to think in terms of events and associations.
	- **Projects like MyLifeBits and Microsoft SenseCam**: These systems aim at “total recall” by collecting rich media—from photos to annotations—though studies reveal that having vast, uncurated data does not necessarily enhance recall.
	  
	  Furthermore, systems such as **Presto** and **Haystack** are introduced as innovative PIM solutions. Presto redefines documents as placeless entities—separating content from storage and embracing dynamic, predicate-based classification. Haystack, on the other hand, uses RDF to create personalized, context-driven information environments.
	  
	  ---
	- ### 5. Structural Linking, Cross-Media Associations, and the OC2 Framework
	  
	  A core “blue‐incorporated” contribution of the lecture is the discussion of linking techniques across media and the development of formal models for hypermedia systems:
	- **RSL Hypermedia Metamodel**: This concept underlines the significance of navigational, structural, and associative links that bind digital documents and multimedia resources into a fluid space.
	- **Object-Concept-Context (OC2) Framework**: Presented with layered diagrams, the OC2 model distinguishes between the physical objects (and their structural links), the abstract concepts (the semantic layer), and the contextual relationships that give meaning to associations. This framework facilitates dynamic, cross-media connections and enriches the searching and browsing experience.
	  
	  ---
	- ### 6. Next-Generation Presentation and Interaction Platforms
	  
	  The latter portion of the lecture shifts focus to practical applications—systems that embody these theoretical advances:
	- **MindXpres Presentation Platform**: Unlike traditional slideware, MindXpres offers a limitless canvas with a zoomable interface, non-linear navigation, and associative linking. This platform dissolves the rigid boundaries of slide-based presentations, enabling rich media incorporation and dynamic content reuse.
	- **Interactive Tools for Source Code and Data Visualisation**: These include plug-ins for visualizing source code within presentations and methods for interactive, narrative data visualisation. The goal is to support knowledge transfer in a manner that leverages cross-media associations rather than linear, static formats.
	  
	  ---
	- ### 7. Bridging the Paper–Digital Divide
	  
	  Throughout the lecture, a recurring theme is the challenge of reconciling our traditional paper-based habits with the potential of digital, cross-media environments. The blue-highlighted key points underscore that digital systems need not merely mimic paper; they can—and should—offer far richer, more interactive, and fluid experiences. Such systems embrace dynamic categorization, multi-view representations (as seen in the various PimVis views), and seamless linking across disparate information repositories.
	  
	  ---
	- ### 8. Assignments, Exercises, and Further Inquiry
	  
	  To cement these ideas, Professor Signer assigns further reading and hands-on exercises (e.g., with Phidgets, Arduino, and ESP32) that encourage students to experiment with cross-media information spaces and prototype novel interaction paradigms. Moreover, a curated list of references—spanning seminal works from Bush to contemporary hypermedia research—provides the academic foundation behind these design innovations.
	  
	  ---
	- ### Conclusion
	  
	  In essence, the lecture redefines how we conceptualize and interact with information. By incorporating the “blue‐incorporated” key points, Professor Signer advocates for a move away from static, paper-like digital documents toward dynamic, multilayered, and context-aware interfaces. This vision supports more natural, flexible, and engaging interactions—ultimately bridging the gap between human information practices and modern digital architectures.
- lecture 3
  collapsed:: true
	- ### 1. The Foundations of Multimodal Interaction
	  
	  The lecture begins by framing human–machine communication as inherently multimodal. In contrast to traditional graphical user interfaces (GUIs), which rely on a single event stream (usually mouse and keyboard), multimodal systems blend multiple input and output channels. This stems from the observation that humans naturally communicate and perceive through various senses. The slide underscores that modalities include:
	- **Visual (sight)**
	- **Auditory (hearing)**
	- **Olfactory (smell)**
	- **Gustatory (taste)**
	- **Tactile (touch)**
	- **Vestibular (balance)**
	  
	  This diversity of perceptual channels sets the stage for designing interfaces that tap into our innate abilities for spatial reasoning, gesture, speech, and so on.
	  
	  ---
	- ### 2. Historical Milestones and the “Put-that-there” Paradigm
	  
	  One of the earliest influential examples in multimodal research is R.A. Bolt’s seminal “Put-that-there” demonstration from 1980. In this system:
	- **Speech** is used to express the semantic content of a command.
	- **Gesture** (such as pointing) supplies spatial or locational information.
	  
	  By combining these modalities, the system achieves a level of richness that neither speech nor gesture could provide alone. Although this interaction model was originally built on a mouse-oriented metaphor, it laid the groundwork for understanding how complementary modalities can enhance command accuracy and user experience.
	  
	  ---
	- ### 3. Core Advantages and Interaction Characteristics
	  
	  Multimodal interfaces offer several distinct benefits that enhance user experience:
	- **Improved Accessibility:** By leveraging multiple input channels, these systems accommodate users with different sensory and motor capabilities.
	- **Natural Interaction:** Users interact with systems more intuitively, mirroring real-world communication (e.g., speaking while gesturing).
	- **Enhanced Robustness:** The fusion of information from different modalities can help disambiguate commands and reduce errors.
	- **Flexible Personalization:** Systems can adapt to users’ unique integration patterns—whether sequential or parallel—allowing for a more fluid interaction compared to traditional interfaces.
	  
	  A concrete instance of these benefits is demonstrated by the SpeeG2 system. In SpeeG2:
	- The user speaks naturally and sees probable words displayed on the screen.
	- Hand gestures are then used to correct or confirm the recognized words, resulting in an improved recognition rate (up to 21 words per minute).
	  
	  ---
	- ### 4. Fusion and Fission in Multimodal Systems
	  
	  A significant portion of the lecture is devoted to how different input streams are merged or “fused” within a multimodal system. Fusion can occur at several levels:
	- **Data-Level Fusion:** Combining raw data streams (e.g., video signals from different angles). This level is highly detailed but sensitive to noise.
	- **Feature-Level Fusion:** Merging extracted features, such as pairing lip movements with speech signals.
	- **Decision-Level Fusion:** Combining high-level interpretations (e.g., using both speech and gesture to infer user intent). This level tends to be more robust against errors.
	  
	  On the flip side, **multimodal fission** involves coordinating multiple output channels (such as auditory and visual feedback) to construct a coherent response for the user. The process includes selecting content, choosing modalities, and ensuring that outputs across channels deliver a unified message.
	  
	  ---
	- ### 5. Debunking Misconceptions: Ten Myths of Multimodal Interaction
	  
	  The lecture also addresses common myths and misconceptions about multimodal systems:
	  1. That users will always engage using multiple modalities.
	  2. That speech and pointing dominate every multimodal interaction.
	  3. That input signals must overlap in time.
	  4. That speech is always the primary mode.
	  5. And several others regarding redundancy, integration patterns, and content equivalence.
	  
	  Understanding these myths is key to designing systems that accommodate the real, mixed nature of human interaction—where both unimodal and multimodal commands may be interleaved based on task and context.
	  
	  ---
	- ### 6. Formal Models and the CARE Properties
	  
	  To provide a structured approach to evaluating multimodal interaction, the lecture introduces formal models—especially the CARE properties (Complementarity, Assignment, Redundancy, and Equivalence) proposed by Coutaz and colleagues. These properties help:
	- **Complementarity:** Recognize when multiple modalities are necessary to achieve a complete command.
	- **Assignment:** Determine when a single modality suffices.
	- **Redundancy:** Understand when multiple modalities provide the same information without extra expressive power.
	- **Equivalence:** Offer users a choice between modalities to achieve the same outcome.
	  
	  An example of equivalence is seen in interfaces where the user may either speak a command or click on an icon to achieve the same function.
	  
	  ---
	- ### 7. Multimodal Interaction Frameworks
	  
	  Several software frameworks support the development of multimodal interfaces by providing the necessary tools for fusion and interpretation:
	- **Squidy:** A stream-based framework that uses a graphical pipeline of components and filters.
	- **OpenInterface:** A component-based European project offering an online library of devices and modalities.
	- **HephaisTK:** Focuses on event-based recognition and employs decision-level fusion.
	- **Mudra:** A hybrid framework that combines multiple fusion strategies using a declarative, rule-based approach.
	  
	  These frameworks enable rapid prototyping and integration of new modalities, making them vital for advancing the field of multimodal interaction.
	  
	  ---
	- ### 8. Final Thoughts
	  
	  The “blue‐incorporated” elements of the lecture highlight how multimodal interaction pushes beyond traditional interfaces to more naturally engage users through a blend of modalities. By understanding both the technical frameworks (fusion/fission, CARE properties) and the practical applications (like SpeeG2 and Bolt’s “Put-that-there”), we gain insight into designing systems that are robust, accessible, and aligned with natural human communication.
	  
	  This comprehensive overview not only maps out the current landscape but also paves the way for future research in dynamic and rich multimodal environments. Would you like to delve into any of these areas in more depth—perhaps exploring one of the fusion techniques or examining a particular framework further?
- lecture 4
  collapsed:: true
	- ## 1. Historical Evolution of Writing Tools
	  
	  The lecture begins by tracing the evolution of writing instruments over roughly 40,000 years. It explains that—from the earliest cave paintings made with stones to sophisticated styluses—the development of pens has paralleled that of the surfaces on which we write. Throughout history, every change in writing technology (from reed and quill pens to fountain, ballpoint, and marker pens) was met with an equally lengthy period of adjustment. For example, even when paper first emerged, there was a long period before it was fully adopted. This coevolution means that users today carry deep-seated expectations and habits when interacting with pen and paper.
	  
	  ---
	- ## 2. The Myth of the Paperless Office
	  
	  A central theme of the lecture is the long-held prediction of a paperless office—a future where electronic documents would replace printed ones completely. Despite decades of forecasts envisioning a shift to entirely digital workspaces, the reality is quite different. Paper continues to be integral because of its tangible affordances: it is lightweight, flexible, and offers a form of navigation (such as physically flipping through pages) that digital systems have yet to fully replicate. The presentation invites us to reconsider the “imminent” revolution in work practices, suggesting that the enduring nature of paper may be due to its inherent qualities rather than mere tradition.
	  
	  ---
	- ## 3. Affordances of Pen and Paper
	  
	  One of the strongest arguments in the presentation is that both paper and pens possess unique affordances that have shaped human-computer interaction.
	- **Paper** is characterized as being light, flexible, and inexpensive. It is also robust, available in varying degrees of opacity or transparency, and offers high resolution for fine details. The physicality of paper allows for actions such as grasping, folding, tearing, and spatial arrangement—the very interactions that support collaborative work and dynamic information processing. Its localized nature, however, means that the document must be physically present to be interacted with, creating both benefits and limitations.
	- **Pens**, meanwhile, are emphasized for their durability, light weight, mobility, and versatility (being available in color and often erasable). They can function on a variety of surfaces and add a dimension of personalization to written communication. These inherent qualities have led to an enduring partnership between pen and paper in both educational and professional settings.
	  
	  ---
	- ## 4. Replacing Pen and Paper?
	  
	  The lecture also addresses the current debate over whether digital alternatives can—or should—replace traditional pen and paper. Several studies are cited to support the continued importance of handwriting. Notably, research indicates that generative note-taking by hand can be more beneficial for learning compared to transcribing notes on a laptop. A 2024 study by Van der Weel and colleagues found that handwriting (in contrast to typewriting) facilitates widespread brain connectivity. Moreover, educational trends in countries like Sweden emphasize a return to handwriting, reinforcing the idea that, despite the digital revolution, the tactile and cognitive benefits of traditional methods remain significant.
	  
	  ---
	- ## 5. Digital Pen and Paper Applications
	  
	  A further section of the lecture explores how pen-based interaction has been extended into the digital realm. Various applications have leveraged the natural feel of pen and paper, including:
	- **Enhanced Reading:** Interactive paper maps and festival brochures that combine physical documents with digital annotations.
	- **Enhanced Writing:** Tools such as research notebooks and even specialized systems for tasks like mammography annotation that benefit from the natural precision of handwriting.
	- **Collaborative Interfaces:** Tools like PaperPoint, which enable non-linear presentations and real-time collaborative editing, illustrate how traditional and digital methods can blend to create fluid, dynamic user interfaces.
	  
	  Additionally, the lecture discusses digital ink processing—the technological side of capturing pen input. This includes parsing proprietary pen formats and transforming data (position, timestamp, force, tilt) into standardized forms like Ink Markup Language (InkML), which facilitates interoperability and consistent processing across devices.
	  
	  ---
	- ## 6. Innovative Hardware, Materials, and Integration
	  
	  Moving beyond applications alone, the lecture considers emerging trends in hardware and materials that are designed to bridge the paper–digital divide. There are demonstrations of concepts that integrate physical paper with digital feedback. For instance, systems like HoloDoc deliver layered, mixed-reality feedback via head-mounted displays (such as HoloLens), while spatially aware projections and printed electronics open new avenues for interaction. These innovations are efforts to overcome some of the inherent limitations of paper (like its inability to provide immediate, dynamic feedback) while preserving its valuable affordances.
	  
	  ---
	- ## 7. Conclusions and Future Directions
	  
	  In closing, the lecture acknowledges that while pen-based input is not universally optimal—for many tasks, traditional keyboards or other input methods may still be faster—it remains indispensable for its unique utility and the way it aligns with human habits. The challenges highlighted include reconciling multimodal interactions (combining pen, digital, and tactile feedback) and meeting users’ entrenched expectations for what pen and paper should do. The discussion provokes further inquiry into designing more adaptive interfaces that respect the inherent advantages of traditional tools while embracing the innovations of the digital age.
- lecture 5
  collapsed:: true
	- ### **Defining the Tabletop Interface**
	- **Concept and Design Philosophy:**  
	  Originally coined around 2001, a “tabletop” interface builds on the everyday notion of a physical table. The design relies on users’ existing mental models of tables as shared, horizontal surfaces. This familiarity encourages natural interaction forms—such as placing objects, grouping items, or splitting work into individual versus shared zones—while simultaneously introducing novel interaction modalities like multi-touch, hand gestures, and tangible manipulations.
	- **Typical Applications and Limitations:**  
	  Although tabletops are often used for dedicated tasks such as viewing multimedia content, brainstorming, and advanced data visualization, they have yet to become all-purpose computing devices. One key limitation is the lack of traditional peripherals (keyboard, mouse) that many users rely on for productivity tasks. Furthermore, ergonomic issues—such as potential neck strain or difficulties with reachability when used as a primary input method—underscore the need for adjustable and thoughtfully designed interfaces.
	  
	  ---
	- ### **Multi-User Interaction and Workspace Organization**
	- **Multi-User Tabletop Interfaces:**  
	  A core strength of the tabletop is its ability to support collaboration. The interface is large enough for several users to interact simultaneously, with studies showing that users naturally divide the surface into distinct territories:
		- **Personal Territories:** Areas immediately in front of users, used for private or individualized tasks such as reading or writing.
		- **Group Territories:** Central zones where collaborative activities and shared tasks occur.
		- **Storage Territories:** Peripheral spaces for holding or organizing both task-related resources and even non-task items.
		  
		  The lecturer underscores several guidelines for designing these interfaces—for instance, ensuring that all actions are visible and that each territorial division is functionally optimized (e.g., localized reading/writing support). These principles are central for minimizing conflict and enhancing smooth interactions among multiple users.
		  
		  ---
	- ### **Enabling Technologies and Hardware Considerations**
	- **Touch Sensing and Display Integration:**  
	  The building blocks of an interactive tabletop include robust touch sensor technology, a display (which might be a projector, LCD, or OLED), and supporting software that transcends traditional WIMP (Windows, Icons, Menus, Pointer) paradigms. The software must handle simultaneous inputs from various users and fingers, often requiring complex event management and support for rotation of display areas.
	- **Types of Touch Panel Technologies:**  
	  The document provides a detailed look at several key touch technologies, each with its own strengths and tradeoffs:
		- **Resistive Touch Panels:**  
		  Built from two conductive layers separated by an insulating film, resistive panels are low-cost and power-efficient. They work with a stylus or even gloved input, though the extra layers tend to reduce display clarity.
		- **Surface Capacitive Touch Panels:**  
		  Featuring a uniform transparent conductive coating with electrodes at the corners, these panels provide high positional accuracy and superior display quality compared to resistive panels. However, they can be less effective at accurately detecting multiple touches.
		- **Projected Capacitive Touch Panels:**  
		  These utilize a sensor grid behind the front layer to support multi-touch with high accuracy. While popular in modern smartphones, their scalability to large surfaces can be limited, partly due to slower signal transmission.
		- **Surface Acoustic Wave (SAW):**  
		  SAW technology uses ultrasonic waves to detect touch—it has the advantage of no extra top layers, thus preserving display clarity. Yet, it is often limited to dual-touch displays and may require a larger frame area for transducers.
		- **Frustrated Total Internal Reflection (FTIR):**  
		  FTIR leverages infrared light that is internally reflected within the surface material. When a user’s finger touches the surface, the touch “frustrates” the total internal reflection, and computer vision algorithms detect the location of the contact. This technology can be paired with back projection to enrich the interactive experience.
		- **Diffused Illumination (DI):**  
		  By positioning infrared light behind the projection surface, DI can detect both fingers and objects (sometimes using shape cues or fiducial markers), thereby broadening the interaction possibilities beyond just finger touches.
		  
		  ---
	- ### **Showcase of Interactive Tabletop Systems and Innovations**
	  
	  The lecture highlights several pioneering systems and prototypes that illustrate the evolution and potential of tabletop interfaces:
	- **DigitalDesk:**  
	  An early system developed by Pierre Wellner that transformed an ordinary desk into a digital workstation with integrated camera-based tracking and projection.
	- **DiamondTouch Table:**  
	  Developed at Mitsubishi Electric Research Laboratories (MERL), this system uniquely identifies who is touching the surface by using capacitive coupling from sensors embedded in users’ chairs.
	- **Jeff Han’s Multi-Touch Table:**  
	  A landmark project that uses a refined FTIR setup to deliver a simple, cost-effective multi-touch interactive surface.
	- **BendDesk:**  
	  An innovative concept that blends a vertical and a horizontal multi-touch surface. Using multiple cameras and projectors along with FTIR-based tracking, BendDesk demonstrates how the interface can adapt to different orientations.
	- **Microsoft PixelSense (Samsung SUR40):**  
	  A 40-inch LED-backlit LCD that integrates multi-touch capability with a unique sensor system where individual pixels double as infrared detectors.
	- **ReacTIVision:**  
	  An open-source framework for tangible interaction that employs fiducial markers, allowing physical objects to be tracked and manipulated on the tabletop surface.
	- **Pen and Touch Integration:**  
	  Beyond finger touch, the presentation explores systems that integrate pen input simultaneously with touch. Here, the non-dominant hand can signal when the pen is in “mode” for writing or other commands, offering an advanced alternative to conventional on-screen widget interfaces.
	  
	  Additional systems—such as interactive spaces like the “we-inspire Room,” immersive computing platforms like HP Sprout Pro, and all-in-one creative systems like Microsoft Surface Studio 2—demonstrate how these underlying technologies are being applied. There is even discussion of emerging display innovations like OLEDs that promise flexible, thin, and highly responsive surfaces, further pushing the boundaries of what a tabletop system can be.
	  
	  ---
	- ### **Emerging Concepts and Futuristic Directions**
	- **Future-Ready Displays:**  
	  The presentation also touches on revolutionary display concepts, such as OLED technology. OLEDs are noted for their flexibility, rapid response times (up to 1000 times faster than traditional LEDs), and the potential for cost-effective production. These qualities make them ideal candidates for integration with next-generation multi-touch systems, paving the way for even more dynamic and adaptable interactive surfaces.
	- **Speculative and Experimental Designs:**  
	  Concepts like the “Windowless Plane”—a futuristic design that hints at using camera streams and dynamic windows without physical edges—illustrate the visionary side of interactive tabletop research. Other experimental projects, such as the AquaTop Display, explore entirely new materials (like water surfaces) as interactive canvases, opening the door to fresh approaches and applications.
- lecture 6
  collapsed:: true
	- ## **2. Defining Gestures and Their Types**
	  
	  At the heart of the lecture is a clear definition: a gesture is a form of non‐verbal or non‐vocal communication where visible bodily actions (such as movements of the hands, face, or other parts) convey specific messages. Drawing from A. Kendon’s work, the presentation explains that gestures can be separated into three functional groups: semiotic, ergotic, epistemic
	- **Semiotic Gestures:** These are used explicitly to communicate meaning. Within this group, further classifications are given: (**sdip**)
		- **Symbolic Gestures (Emblems):** Culture-specific signs like the “OK” sign that hold a single, clear meaning.
		- **Deictic Gestures:** Pointing actions that direct attention to a location or object.
		- **Iconic Gestures:** Movements that illustrate properties (such as size, shape, or orientation) of the object being described.
		- **Pantomimic Gestures:** Movements that mimic handling or using an object, even when the object is absent.
		  
		  The focus is largely on semiotic gestures for human‐computer interaction, emphasizing the need for gestures to be intuitive, easy to perform, and unambiguous.
		  
		  ---
	- ## **3. Gesture Recognition Devices and Techniques**
	  
	  The lecture thoroughly surveys a wide range of devices used for capturing gestures. These include: (**WAVSER**)
	- **Wired Gloves (Data Glove/Cyberglove):**  
	  Sensors embedded in gloves (using magnetic or inertial tracking) capture both hand and finger positions. Although sometimes offering haptic feedback, these devices are increasingly replaced by camera-based methods due to their physical constraints.
	- **Accelerometers:**  
	  Small, cost-effective sensors that measure acceleration. They are deployed in a range of consumer electronics—from smartphones (for screen orientation and stabilization) to gaming devices like the Nintendo Wii Remote. Their strength lies in tracking dynamic motion, but they are less suited for recognizing static postures.
	- **Vision-Based Systems (Camcorders/Webcams):**  
	  Using computer vision, these systems capture gestures via video. Although hardware is inexpensive and widely available, they must first detect the human body or parts of it before gesture recognition can occur, and they often struggle with depth information.
	- **Skeleton Tracking with Range Sensors:**  
	  Popularized by devices such as the Microsoft Kinect (introduced around 2010), these systems combine infrared depth sensing with RGB cameras to capture full-body gestures in three dimensions. They offer robust skeletal tracking using fusion of depth data, making them excellent for whole-body gesture recognition.
	- **Electromyography (EMG) and Wearables:**  
	  Devices like the Myo bracelet incorporate muscle sensors, gyroscopes, accelerometers, and magnetometers to detect subtle gestures and even sign language. These wearables offer haptic feedback and open up possibilities for fine gesture-based control.
	- **Radar-Based Recognition (Project Soli):**  
	  An emerging technology that leverages radar to detect fine, millimetric motions. Its integration into products like the Pixel 4 exemplifies the trend toward miniaturization and precise detection.
	  
	  Each of these technologies comes with its own set of tradeoffs in terms of accuracy, scalability, cost, and usability.
	  
	  ---
	- ## **4. Gesture Recognition Algorithms**
	  
	  A significant portion of the lecture is dedicated to the algorithms that drive gesture recognition. Three broad families are outlined: (**TMR**)
	- **Template-Based Algorithms:**  
	  Methods such as the Rubine algorithm, Dynamic Time Warping (DTW), and the $1/$N recogniser use predefined gesture templates. The Rubine algorithm, in particular, is examined in detail. It represents a gesture as a vector of sample points and extracts 13 features (e.g., cosine and sine of the initial angle, bounding box diagonals, total gesture length, total angle traversed, maximum speed, and duration) to statistically classify gestures.
	- **Machine Learning-Based Algorithms:**  
	  Approaches using neural networks, Hidden Markov Models (HMM), and other learning frameworks process the vectorised, spatio-temporal data from gestures. These methods require extensive training data to perform accurately.
	- **Rule-Based Approaches:**  
	  Some systems combine rule-based techniques with statistical learning to benefit from the strengths of both, sometimes within frameworks like LADDER or integrated systems such as Mudra.
	  
	  The lecture underlines that selecting an appropriate recognition technique is crucial, as each comes with challenges regarding training data needs and real-time application constraints.
	  
	  ---
	- ## **5. Gesture Spotting and Segmentation**
	  
	  Unlike discrete inputs (like button presses), gestures in continuous, always-on environments (such as with Kinect) do not have clearly defined start and end points. The lecture details methods for gesture spotting or segmentation, including:
	- **Using an Additional Modality:**  
	  For example, pressing a button or using a voice command to indicate the start of a gesture. Although effective, it may not feel natural.
	- **Continuous Gesture Spotting:**  
	  The approach involves continuously monitoring and applying spatio-temporal constraints to segment a stream of movement into potential gestures. Recognised patterns are then fed into the gesture recognizer for classification. Researchers such as Hoste et al. (2013) are cited as having advanced these methods.
	  
	  ---
	- ## **6. Developing Effective Gesture Vocabularies**
	  
	  Creating a gesture vocabulary for user interfaces is more challenging than it seems. The lecture emphasizes:
	- **Ergonomic and Usability Considerations:**  
	  Gestures should be easy to perform, remember, and should match the function they represent both metaphorically and iconically. Issues like the “gorilla arm” (user fatigue) must be avoided.
	- **Distinctiveness and Scalability:**  
	  Gestures must be visually and dynamically distinct to ensure reliable recognition. A large set of similar gestures can create confusion and reduce both recognition accuracy and ease of learning.
	- **Specialized Techniques:**  
	  For instance, shape writing techniques are discussed as methods for text input on touchscreens, where a single continuous stroke corresponds to a word. The “fat finger” problem is also addressed, with solutions like enlarging touch targets, providing real-time feedback, or applying adjustments based on user perception.
	  
	  Additionally, the lecture details how some commercial systems (including those developed by Microsoft) use custom gesture sets for operations such as cut, copy, paste, undo, and redo, further showcasing the diversity of gesture vocabularies proposed for different applications.
	  
	  ---
	- ## **7. Integrated Frameworks and Practical Tools**
	  
	  The iGesture framework is introduced as an open-source tool allowing researchers and developers to experiment with gesture recognition:
	- **iGesture Workbench:**  
	  This tool provides an environment for creating, testing, and evaluating gesture sets and algorithms. It supports multiple input modalities—ranging from digital pens and tablet PCs to devices like the Wii Remote—thereby facilitating rapid prototyping and multimodal gesture fusion.
	- **Evaluation Tools and Architectures:**  
	  The lecture includes architectural overviews and evaluation methodologies that help in understanding how gesture recognition components (from data capture to pattern classification) are integrated in a system.
	  
	  ---
	- ## **8. Usability Challenges and a Call for Better Design**
	  
	  Despite technological advances, the presentation argues that gestures have not always lived up to usability expectations. Usability tests on gesture-based interfaces have revealed critical issues:
	- **Lack of Clear Signifiers and Feedback:**  
	  Many systems do not offer intuitive visual cues or adequate feedback for gestures, leading to user confusion (e.g., ambiguous “back” actions on mobile devices).
	- **Consistency and Discoverability Issues:**  
	  The absence of standardized guidelines across platforms makes it difficult for users to learn and predict gesture functions. This inconsistency can lead to frustration and a disconnect between user intent and system response.
	  
	  The lecture criticizes the tendency among some companies to disregard long-established HCI principles, urging designers to incorporate clear, intuitive, and ergonomically sound gestures that build on decades of interaction research.
	- **Visibility, feedback, consistency and standards, discoverability, scalability, reliability, lack of undo**
	- ---
	- ## **9. Conclusion and Forward Look**
	  
	  In closing, Professor Signer emphasizes that while gesture-based interaction offers a pathway toward more natural and intuitive computer interfaces, its successful implementation relies on careful consideration of hardware limitations, algorithm selection, effective gesture vocabularies, and—most critically—a commitment to usability. Challenges such as gesture segmentation, reliable recognition, and user fatigue still need to be addressed. The lecture also sets the stage for further inquiry by assigning related readings (such as critical papers on gestural interfaces), urging designers and researchers alike to refine these technologies to better serve natural human communication.
- lecture 7
  collapsed:: true
	- ### 1. Introduction to Tangible Interaction
	  
	  The lecture reimagines the way humans interact with digital information by embedding computational processes into physical objects and environments. Rather than confining data and control to flat, on-screen elements, tangible interaction advocates for interfaces where the physicality of everyday objects becomes the medium for digital communication. This approach seeks to leverage our innate skills in handling objects, spatial reasoning, and direct physical manipulation.
	  
	  ---
	- ### 2. Core Concepts and Theoretical Foundations
	  
	  **Affordances and Discoverability:**  
	  Drawing on foundational work by Gibson and Norman, the lecture emphasizes that every physical object carries inherent “affordances”—clues about its usage. These affordances enable seamless interactions because users can intuitively understand how to manipulate an object purely by its shape or design. Good tangible interaction design, therefore, externalizes internal computer representations onto forms that are directly recognizable and usable.
	  
	  **Tangible Interaction as an Umbrella Term:**  
	  The lecture defines tangible interaction as an inclusive term covering graspable user interfaces, tangible user interfaces (TUIs), and embodied interaction. In all these cases, digital information isn’t abstracted on a monitor; it is manifested in physical matter—whether through direct manipulation of objects, whole-body gestures, or embedded environmental cues.
	  
	  ---
	- ### 3. Exemplary Systems and Prototypes (MGTMUS)
	  
	  **Marble Answering Machine:**  
	  One of the early examples presented is the Marble Answering Machine. This system uses familiar physical marbles to represent incoming messages. Its design is elegant and simple: a one-step action reveals audio messages, relying on aesthetic appeal and intuitive physicality rather than complex controls. However, the system also highlights practical design considerations such as robustness and context sensitivity (e.g., public versus private usage).
	  
	  **Graspable User Interfaces:**  
	  Moving forward, the lecture revisits work from 1995 on graspable user interfaces. These systems let users manipulate “bricks” that are directly coupled with virtual objects (often on large interactive surfaces like the ActiveDesk). By facilitating two-handed and multi-point interactions, such systems tap into our natural dexterity and spatial skills, simultaneously externalizing digital data for richer collaboration and interaction.
	  
	  **Tangible Bits and Life Wire:**  
	  Hiroshi Ishii’s concept of Tangible Bits is explored as a way to bridge the digital and physical realms. Projects like Life Wire—where bits from network traffic are made tangible through physical movement, sound, and touch—demonstrate how abstract digital phenomena can be rendered in ways that captivate the senses and reveal hidden dynamics of data flow.
	  
	  **MetaDESK and AmbientROOM:**  
	  Other prototypes such as metaDESK illustrate the integration of tangible tools (phicons, ActiveLENS, instruments) with digital interfaces, creating a blended workspace. AmbientROOM extends this idea by using ambient media—light, shadows, sound, and airflow—to communicate peripheral information, facilitating smooth transitions between focused tasks and background processing.
	  
	  **Urban Planning and Interactive Media:**  
	  The lecture also covers applications like the Urp system, which merges physical models of buildings with interactive simulations for urban planning. Similarly, interactive art installations such as the Sand Noise Music Device show how tangible surfaces (in this case, shifting sand landscapes) can be harnessed to control generative music and artistic expression.
	  
	  **Sifteo Cubes and ZeroN:**  
	  Small, sensor-embedded devices like Sifteo Cubes underscore the potential of tangible gaming and collaborative puzzle-solving, while concepts such as ZeroN illustrate a radical departure from conventional physics by enabling anti-gravity interactions in three-dimensional space, pushing the boundaries of how we perceive and manipulate digital information.
	  
	  ---
	- ### 4. Pushing the Boundaries: Radical Atoms and Dynamic Physicalisation
	  
	  The lecture culminates with forward-looking concepts such as TRANSFORM and Radical Atoms. This vision imagines materials that are computationally reconfigurable—capable of dynamically changing their shape, texture, and even tactile properties in response to digital cues. Such “Radical Atoms” embody a future where every bit of digital data has a corresponding physical manifestation, blurring the line between the virtual and the physical. The Tangible Hologram (TangHo) Platform is presented as a prototype in this direction, using a Lego Mindstorms–based 6DOF arm to explore bidirectional interaction and shape modulation, even as challenges like inverse kinematics and robust hand tracking remain to be solved.
	  
	  ---
	- ### 5. Vision-Driven Design Research
	  
	  A recurring theme in the lecture is the idea that groundbreaking interaction design often arises not from incremental user studies but from bold visions and creative dreams—echoing the spirit of visionaries like Douglas Engelbart. The research into tangible interaction calls for a willingness to experiment with new materials, digital fabrication technologies, and hybrid interfaces despite current limitations, with the expectation that enabling technologies will eventually catch up to these ambitious ideas.
	  
	  ---
	- ### Conclusion
	  
	  Professor Signer’s lecture weaves together historical insights, modern prototypes, and futuristic visions to argue that tangible interaction holds the key to more natural, intuitive, and engaging user interfaces. By moving digital data into the physical realm and harnessing the rich repertoire of human physical skills, we can create systems that are not just tools but extensions of our own bodily and cognitive abilities. In essence, the “blue-incorporated” elements of the lecture highlight how design rooted in physicality and materiality can transform the way we interact with information—making it richer, more immediate, and profoundly human.
- lecture 8
  collapsed:: true
	- ### **1. Mixed Reality and the Reality–Virtuality Continuum**
	  
	  The lecture begins by defining Mixed Reality (MR) within the framework of the Reality–Virtuality continuum, an idea introduced by Paul Milgram and Fumio Kishino in 1994. This continuum ranges from completely real environments to fully virtual ones, with MR occupying the space in between. In this zone, physical and digital objects coexist and interact in real time. Augmented Reality (AR) and Augmented Virtuality (AV, such as digital twins) are two complementary approaches within this spectrum, each blending real and virtual aspects in distinctive ways.
	  
	  ---
	- ### **2. Virtual Reality (VR): Concepts, Challenges, and Applications**
	  
	  **Concept and Immersion:**  
	  VR is presented as an artificial environment experienced through multisensory input—sight, sound, even touch, taste, and smell—delivered by computer systems. The goal is to create an experience that replicates or substitutes real-world conditions. Immersion, or perceptual immersion, is emphasized as the user’s sensation of physically being present in a synthetic world, achieved via panoramic 3D visuals, surround sound, haptic feedback, and other stimuli.
	  
	  **Historical Foundation – The Sword of Damocles:**  
	  The lecture revisits one of the pioneering systems of VR—the Sword of Damocles. Developed by Ivan Sutherland and his student Bob Sproull in 1968, this early head-mounted display provided a simple stereoscopic view of wireframe rooms and incorporated head tracking, demonstrating the foundational principles of VR despite its mechanical constraints.
	  
	  **Applications Across Domains:**  
	  VR technology is not limited to a single field; instead, it finds applications in:
	- **Architecture:** Allowing users to navigate and experience virtual reconstructions of buildings.
	- **Education:** Visualizing and interacting with complex datasets.
	- **Medicine:** Providing training environments for surgeries, including virtual robotic surgery.
	- **Engineering, Military, Entertainment, Sport, Simulations, and Gaming:** Each of these areas leverages VR for both training and immersive experiences that may be too dangerous, expensive, or impractical in the real world.
	  
	  ---
	- ### **3. VR Technologies and Interaction Techniques**
	  
	  **Technologies:**  
	  The lecture details several core hardware approaches:
	- **Large Screens:** Panoramic, cylindrical, or spherical displays that may or may not use stereoscopy.
	- **Binocular Omni-Orientation Monitors (BOOM):** Devices where screens mounted in a box (linked to a multi-link arm) track the user’s head movements.
	- **CAVE (Cave Automatic Virtual Environment):** Room-sized, multi-wall systems that project stereo images for a shared immersive experience.
	- **Head-Mounted Displays (HMDs):** Lightweight devices (such as the Meta Quest 3) that present digital images directly before the eyes, often with embedded displays and lenses to adjust focus.
	  
	  **Navigation and Interaction:**  
	  Interacting with virtual environments involves both navigation (moving within a three-dimensional scene) and interaction (manipulating elements within that scene). Techniques include:
	- **Navigation Methods:** (GLPW)
		- **Grabbing in the Air:** User selects and drags elements within the virtual world.
		- **Lean-Based Velocity:** The direction and speed of movement are controlled by leaning forward or backward.
		- **Path Drawing or Walking in Place:** Methods that utilize input (sometimes even treadmills like the Virtuix Omni) to translate physical movement into virtual motion.
	- **Interaction Techniques:**
		- **Non-immersive approaches:** Utilizing traditional input devices such as a mouse or joystick.
		- **Immersive methods:** Involving wearable solutions that capture limb motion (e.g., through datagloves or optical tracking) or using hand-mapping and ray casting—for instance, projecting a virtual beam from the hand to select objects.
		  
		  ---
	- ### **4. Augmented Reality (AR): Extending Reality Rather Than Replacing It**
	  
	  **Fundamental Concepts:**  
	  In contrast to VR, AR allows users to maintain contact with the real world while overlaying virtual information onto it. The goal is not to replace the physical environment, but rather to supplement it—merging digital imagery seamlessly with reality.
	  
	  **Applications:**  
	  AR finds practical use in:
	- **Maintenance and Architecture:** Where overlaying information on existing structures can aid in repair or design.
	- **Education and Medicine:** Offering interactive, real-time guidance.
	- **Entertainment, Navigation, Gaming, and Advertising:** Enhancing experiences with real-time, context-sensitive digital overlays.
	  
	  **AR Techniques:**  
	  Multiple methods are employed to bring these augmented experiences to life:
	- **Video Compositing:** Overlaying virtual elements on live video feeds in real time (or during post-processing).
	- **Head-Up Displays (HUDs):** Common in both aviation and automotive settings, these displays project data within the user’s direct line of sight.
	- **Direct Projections:** Such as the SixthSense technology that uses miniature projectors and cameras to blend digital and physical realms.
	- **Magic Lens and Magic Mirror Metaphors:** These approaches use mobile devices or reflective interfaces to reveal digital information about real-world objects.
	- **Magic Eyeglass Techniques:** This category includes see-through head-mounted displays that allow a superimposed virtual image over an unobstructed view of reality.
	  
	  ---
	- ### **5. AR Hardware: See-Through Technologies and Beyond**
	  
	  **Optical See-Through HMDs:**  
	  These devices (e.g., Microsoft HoloLens 2 and Magic Leap 2) generate virtual images that are overlaid on semi-transparent surfaces. Head tracking ensures that the virtual graphics remain aligned with the user’s view, merging the digital with the physical seamlessly.
	  
	  **Video See-Through HMDs:**  
	  Devices like the Meta Quest 3, Apple Vision Pro, HTC Vive XR, Varjo XR-4, and PICO 4 Ultra capture real-time video of the external world, then layer virtual content upon it, often enhancing the natural view.
	  
	  **Virtual Retinal Displays (VRDs):**  
	  An innovative approach is exemplified by Google Glass, where light beams are directly projected onto the retina. This method gives the illusion of a floating display in front of the eye while raising interesting questions about privacy and social acceptance—key topics the lecture briefly touches upon.
	  
	  ---
	- ### **6. The WebXR Standard and Future Directions**
	  
	  **WebXR:**  
	  The lecture introduces WebXR as a modern, web-based specification aimed at developing XR applications that function seamlessly across diverse platforms and devices. Its standardized Device API supports not only basic viewing but also integrated features such as hand tracking, spatial audio, haptic feedback, and sophisticated room tracking. This ensures that developers can write an application once and have it run in varied immersive setups.
	  
	  **Future Trends in AR:**  
	  Looking forward, the lecture highlights the possibilities of:
	- **Smarter and Less Distracting Augmentations:** For example, augmented contact lenses could provide a subtler interface.
	- **Improved Tracking Technologies:** As seen with advances in HMDs and sensor-based solutions.
	- **Ongoing Social and Ethical Challenges:** As AR systems become more pervasive, issues such as privacy, safety, and social acceptance (as illustrated by the experience with Google Glass) will be at the forefront.
- lecture 9
  collapsed:: true
	- ### 1. **Foundations and Definitions**
	  
	  **Data physicalisation** is defined as the transformation of abstract data into tangible artifacts whose geometry or material properties encode information. This idea—originally framed by Jansen et al.—highlights that physical objects can serve as information carriers much like a digital display, yet they engage our senses in ways that purely visual systems cannot.
		- **Historical Roots:**  
		  Early data representations include the use of **Mesopotamian clay tokens (8000 BCE)** and the **Quipu (2500 BCE)** from Andean cultures. These artifacts demonstrate that society has long used physical forms to encode, communicate, and reason with quantitative information.
		- **Artistic Meets Functional:**  
		  The lecture distinguishes between standard data physicalisation and **data sculptures**—the latter being artistic artifacts designed not only to display data but to evoke insight and spark discussion about socially relevant issues.
		  
		  ---
	- ### 2. **Domains, Applications, and the Role of Multisensory Engagement**
	  
	  Data physicalisation spans a wide array of fields and applications:
		- **Domains of Application:**  
		  It bridges areas such as information and scientific visualisation, tangible user interfaces (TUIs), shape-changing displays, and even electrical engineering. These cross-disciplinary uses help integrate digital data into everyday physical experiences.
		- **Primary Uses:**  
		  Similar to traditional visualisation methods, data physicalisation is used to **discover** insights, **present** complex datasets in an intuitive way, and even **enjoy** data through interactive installations. This multisensory approach—engaging not just sight but also touch, hearing, and possibly even smell—enhances cognition and communication by allowing us to physically manipulate and explore data.
		- **Advantages:**(TAE)
			- **Tactile Exploration and Embodied Cognition:** Physically interacting with data can lead to deeper understanding through direct manipulation.
			- **Accessibility:** Alternative sensory modalities (e.g., tactile or Braille-based displays) enhance data comprehension for users with visual impairments.
			- **Efficiency:** Studies have shown hands-on physical representations (like dynamic 3D bar charts) can outperform their on-screen counterparts by offloading some cognitive tasks onto perceptual and motor systems.
			  
			  ---
	- ### 3. **Enabling Technologies and Dynamic Physicalisation**
	  
	  The lecture outlines a range of **enabling technologies** that have brought data physicalisation to life:
		- **Digital Fabrication Tools:**  
		  The use of **3D printers, laser cutters, and CNC machines** allows designers to rapidly prototype custom data artifacts that are both functional and aesthetically engaging.
		- **Actuated Tangible Interfaces:**  
		  Beyond static models, modern systems incorporate kinetic elements—such as **shape-changing displays** or objects that modify their texture, temperature, or vibration—to communicate real-time or dynamic data. These technologies include olfactory output devices and haptic feedback systems, which together create a truly multisensory experience.
		- **Dynamic Data Physicalisation:**  
		  A significant challenge is transforming static representations into dynamic ones. Research now focuses on:
			- **Designing Dynamic Affordances:** How physical variables (e.g., temperature, friction, smoothness) can be continuously modulated to reflect data changes.
			- **Formal Frameworks:** Development of conceptual and software frameworks (like the dynamic data physicalisation framework discussed) that facilitate rapid prototyping and evaluation of new designs.
			  
			  ---
	- ### 4. **Prototype Implementations and Emerging Concepts**
		- **TangHo Prototype:**  
		  An exemplar implementation mentioned is the **TangHo**, a Lego Mindstorms–based 6DOF arm. This prototype serves as both an output and input device, capable of providing bidirectional I/O and replacing its feedback sphere to accommodate different non-visual modalities. It is emblematic of how physical interaction systems can merge robotics with data communication.
		- **Radical Atoms:**  
		  Looking toward the future, the lecture revisits Hiroshi Ishii’s vision of **Radical Atoms**—materials that are not only digitally transformable but also physically reconfigurable. This concept anticipates a Material User Interface (MUI) in which digital data can “morph” into physical form, offering a new realm of direct manipulation and seamless human-material interaction.
		- **Research Challenges:**  
		  The field is still evolving, and key challenges remain:
			- Developing robust dynamic frameworks that allow for real-time, data-driven physicalisation.
			- Determining the **perceptual effectiveness** of different physical variables, such as what constitutes the just-noticeable difference in a temperature or a tactile modulation.
			- Creating scalable software frameworks that integrate these dynamic affordances into everyday applications.
			  
			  ---
	- ### 5. **Conclusion: The Future of Tangible Data**
	  
	  Data physicalisation represents a shift from purely digital screens toward immersive, tactile, and multisensory environments. By converting digital data into physical form, we not only make it more tangible but also engage our cognitive processes in novel ways. This approach promises to enrich how we understand, interact with, and derive meaning from complex datasets—ushering in a future where data is as much a physical experience as it is an informational one.
- lecture 10
  collapsed:: true
	- ### 1. **Introduction to Next Generation User Interfaces**
	  
	  The lecture sets the stage by outlining a shift toward smarter environments and interactive systems. Inspired by Mark Weiser’s vision of ubiquitous computing, the discussion centers on embedding sensors and information processing into everyday objects. This evolution aims to simplify technology use by developing interfaces that operate implicitly—that is, they leverage contextual and behavioral cues without explicit user commands.
	  
	  ---
	- ### 2. **Implicit Human-Computer Interaction (iHCI)**
	  
	  At its core, implicit HCI distinguishes itself from traditional explicit interaction by relying on naturally occurring behaviors and sensor data. Instead of requiring users to issue direct commands, systems are designed to observe and anticipate needs, incorporating environmental and personal context into their responses. However, this approach comes with challenges. For example, when the system’s anticipatory actions do not match the user’s expectations (an “awareness mismatch”), trust and user satisfaction can suffer. Addressing this requires designing mechanisms that keep users informed about what factors the system is using and why certain actions are taken.
	  
	  ---
	- ### 3. **Context Awareness and Modelling**
	  
	  A key aspect of the lecture is defining and leveraging **context**—any information that characterizes the situation of an entity (whether a person, place, or object). While many systems focus solely on location (e.g., GPS in car navigation), a robust context-aware system also considers:
	- **Human factors:** who the user is, their current activity, and their social environment.
	- **Physical factors:** location, infrastructure, lighting, weather, and other environmental conditions.
	  
	  For instance, a car navigation system might adjust screen brightness based on daylight, disable certain inputs while driving, or adapt its interface based on current traffic conditions. The lecture also discusses techniques—ranging from rule-based logic to machine learning—that can transform raw sensor data into meaningful contextual information.
	  
	  ---
	- ### 4. **Handling Ambiguity in Implicit Interaction**
	  
	  The lecture raises important questions about error handling in implicit HCI. What happens when the system’s anticipatory actions are “wrong”? Is it truly a system error or rather a failure in communicating the factors involved? To compensate for potential misalignments between system behavior and user expectations, designers are encouraged to:
	- Increase system intelligibility by clearly explaining what the system did and why.
	- Evaluate whether a fully automated solution is appropriate or if a semi-automated, user-involved approach might prevent negative experiences.
	  
	  Providing on-demand explanations (covering “What?”, “Why?”, “Why not?”, “What if?” and “How to?”) is deemed crucial to build trust and ease any confusion.
	  
	  ---
	- ### 5. **The Context Modelling Toolkit (CMT)**
	  
	  An instrumental part of the lecture is the introduction of the **Context Modelling Toolkit (CMT)**. This toolkit offers a multi-layered approach that:
	- Bridges the gap between end-users and expert programmers,
	- Moves beyond simple “if-this-then-that” rules by enabling the creation of reusable “situations” and context models,
	- Uses a client-server architecture in which sensor inputs and application logic are seamlessly integrated through a rule engine (like Drools).
	  
	  CMT thus provides a structured way to reason about context and triggers for implicit actions in smart environments.
	  
	  ---
	- ### 6. **Affective Computing and Emotion Recognition**
	  
	  Recognizing that human emotions profoundly influence interactions, the lecture delves into affective computing as another dimension of implicit HCI. It presents several foundational models for understanding and classifying emotions:
	- **Ekman’s Model:** Identifies six basic emotions (anger, fear, disgust, surprise, happiness, and sadness) that can also serve as labels for recognition algorithms.
	- **Russell’s Circumplex Model of Affect:** Maps emotions along the axes of valence (attractiveness) and arousal (reactivity).
	- **Plutchik’s Wheel of Emotions:** Expands the discussion to include advanced or combined emotions (such as love, optimism, or aggressiveness) by understanding how basic emotions interact.
	- **PAD Model:** Characterizes emotions along three dimensions: pleasure-displeasure, arousal-nonarousal, and dominance-submissiveness.
	  
	  Emotion recognition isn’t limited to facial analysis; it spans multiple modalities that include:
	- **Acoustic features:** such as pitch, intonation, and duration in speech.
	- **Visual features:** like facial expressions analyzed via tools such as the Facial Action Coding System (FACS).
	- **Body gestures and posture:** which convey subtle emotional cues.
	- **Biosignals:** for example, heart rate fluctuations and skin temperature changes.
	  
	  Devices like the Emotiv EPOC Neuroheadset illustrate how non-invasive EEG and other biosensors can capture these signals. Machine learning methods (SVMs, neural networks, etc.) are applied either to individual modalities or to fused data from multiple sources, thereby enhancing the system’s ability to accurately recognize and respond to the user’s emotional state.
	  
	  ---
	- ### 7. **Cross-Device and IoT Interaction**
	  
	  The lecture then expands the discussion to include cross-device interaction and Internet of Things (IoT) applications. In today’s diverse digital ecosystem, it is increasingly important to design systems that seamlessly operate across multiple devices. The **eSPACE** platform is highlighted as a rapid prototyping environment that:
	- Enables end-user authoring of applications,
	- Supports a mash-up of digital and physical components,
	- Facilitates the customized distribution of user interface elements,
	- Translates proprietary IoT rules into high-level models (via NLP-based solutions) for execution across different platforms.
	  
	  This approach aims to ensure that user experiences remain consistent and context-aware regardless of which device or sensor is engaged.
	  
	  ---
	- ### 8. **Best Practices and Guidelines for Implicit HCI**
	  
	  To round out the lecture, several key guidelines for implementing effective implicit interactions are offered:
	- **Start with the User:** Thoroughly investigate what users need or want to do before automating any interactions.
	- **Feature Space Definition:** Clearly outline the parameters and measurable factors that define the context for each application.
	- **Minimize Awareness Mismatch:** Strive to design interactions that keep users informed and reduce surprises in system behavior.
	- **Balance Automation and Control:** Determine whether a fully automated approach or one that maintains some degree of user control will deliver the best experience.
	- **Enhance Intelligibility:** Provide on-demand explanations based on sensory inputs and contextual reasoning, which is especially vital when using complex, machine learning–based approaches.
- lecture 11
  collapsed:: true
	- ### 1. **Overview & Motivation**
	  
	  The lecture opens by situating Human-AI Interaction within the realm of next-generation user interfaces. Professor Signer emphasizes that modern AI is not simply about automating tasks—it is about augmenting human capabilities.
	- **Applications Highlighted:** Conversational agents, recommender systems, augmented/virtual reality, and social robots.
	- **Key Insight:** While AI systems can save time and reduce errors through consistency, their probabilistic nature means they can also change behavior over time, which poses unique challenges for design and user trust.
	  
	  ---
	- ### 2. **Challenges in Human-AI Interaction**
	  
	  A central theme is the set of challenges that arise when integrating AI into user interfaces. These include:
	- **Transparency and Explainability:** Users must understand what an AI system can do and why it behaves in a particular way.
	- **Responsiveness and Adaptivity:** AI should act at appropriate times and adapt to the user’s current context without being intrusive.
	- **Privacy, Data Quality, and Ethics:** Systems must avoid unintended biases and ensure fairness, preventing harm, deception, or discrimination.
	  
	  ---
	- ### 3. **Guidelines for Effective Human-AI Interaction**
	  
	  The lecture lays out 18 detailed guidelines (G1–G18) that aim to bridge the gap between AI capabilities and user expectations. These recommendations are designed to ensure that the interaction remains both intuitive and trustworthy:
	- **G1: Make Clear What the System Can Do**  
	  Clearly communicate system capabilities to manage user expectations. For instance, avoiding inflated perceptions—as seen in the misleading expectations from labels like “Autopilot.”
	- **G2: Make Clear How Well the System Can Do What It Can Do**  
	  Offer performance metrics or descriptions so that users understand the reliability and limitations of the service.
	- **G3: Time Services Based on Context**  
	  The system should determine the optimal moments for action based on a user’s current task and surrounding environment.
	- **G4: Show Contextually Relevant Information**  
	  Present information tailored to the user’s current context (e.g., location-based recommendations) while balancing privacy concerns.
	- **G5: Match Relevant Social Norms**  
	  Align the system’s tone and behaviors with the user’s cultural or social expectations. For example, varying levels of formality can affect perceptions of friendliness or appropriateness.
	- **G6: Mitigate Social Biases**  
	  Employ responsible AI toolkits (e.g., Fairlearn, error analysis techniques) to ensure that the language and behavior of the AI do not reinforce harmful stereotypes or biases.
	- **G7: Support Efficient Invocation**  
	  Make it straightforward for users to activate the AI’s services when needed—much like enabling an AI-powered writing assistant with a simple command.
	- **G8: Support Efficient Dismissal**  
	  Likewise, users should find it easy to dismiss or ignore AI interventions that are not relevant to their current needs.
	- **G9: Support Efficient Correction**  
	  When the AI makes an error, users should be able to correct or refine the output effortlessly, such as manually adjusting a route on a navigation app.
	- **G10: Scope Services When in Doubt**  
	  In cases of uncertainty about user intent, the system should either disambiguate by asking for clarification or temporarily limit its response to avoid erroneous actions.
	- **G11: Make Clear Why the System Did What It Did**  
	  Offer both global and local explanations for decisions to boost intelligibility and build user trust—sometimes even including “what if” analyses.
	- **G12: Remember Recent Interactions**  
	  Incorporate a short-term memory to allow for natural, context-rich interactions (like referencing earlier parts of a conversation with a voice assistant).
	- **G13: Learn from User Behavior**  
	  Personalize experiences over time by adapting to individual usage patterns—similar to how a writing assistant learns your style.
	- **G14: Update and Adapt Cautiously**  
	  Introduce changes in a controlled way to prevent disrupting the user’s experience, especially when an AI system has been performing well.
	- **G15: Encourage Granular Feedback**  
	  Enable users to provide detailed, actionable feedback so that the system can continuously refine its behavior.
	- **G16: Convey Consequences of User Actions**  
	  Transparently communicate how specific user actions will influence future interactions or system responses.
	- **G17: Provide Global Controls**  
	  Allow users to set overarching preferences (e.g., privacy settings, monitoring options) that govern the AI’s behavior across applications.
	- **G18: Notify Users About Changes**  
	  Keep users informed about significant updates or modifications to the system’s capabilities, ensuring they can adjust their expectations accordingly.
	  
	  ---
	- ### 4. **A Human-Centred Framework for AI**
	  
	  Beyond the guidelines, the lecturer introduces the Human-Centered Artificial Intelligence (HCAI) framework, which is visualized as a two-dimensional space balancing:
	- **Human Control:** The degree to which users can govern or influence the system.
	- **Computer Automation:** The level of autonomous, algorithmic decision-making.  
	  
	  The ideal region is one where both are high—a scenario that promises reliable, safe, and trustworthy applications. Examples such as thermostats, elevator controls, and smartphone cameras illustrate how this balance enhances usability by offering rapid feedback, reversible actions, and error prevention while still leveraging sophisticated automation.
	  
	  Additionally, the **Prometheus Principles** are mentioned as best practices, focusing on:
	- Consistent interfaces that reflect a clear mapping between user intent and system action.
	- Continuous and informative feedback that keeps the user aware of system status.
	- Rapid, incremental, and reversible actions to prevent errors and facilitate recovery.
	  
	  ---
	- ### 5. **Practical Applications and References**
	  
	  Throughout the lecture, real-world examples bring these concepts to life:
	- A **thermostat** that shows both the current and target temperature while allowing users to adjust settings manually, with the system learning from patterns over time.
	- **Elevators** that blend automation with human oversight to ensure safe and efficient operation.
	- A **smartphone camera** that automatically corrects for shaking hands while also permitting manual adjustments when desired.
	  
	  The lecture concludes by citing influential work and toolkits—from Microsoft’s HAX Toolkit to IBM and Google’s design principles for AI—all of which underscore the need for human-centered design in an era where AI is deeply interwoven into our everyday interfaces.